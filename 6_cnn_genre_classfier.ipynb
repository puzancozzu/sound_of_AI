{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "what we will do:\n",
        "\n",
        "\n",
        "* create train, validation and test sets\n",
        "\n",
        "* build the CNN net\n",
        "\n",
        "* compile the network\n",
        "\n",
        "* train the CNN\n",
        "\n",
        "* evaluate the CNN on the test set\n",
        "\n",
        "* make prediction on sample\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6oUffX4-7lxS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JjIAy2Lh7PX7"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import tensorflow.keras as keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Flatten, Dense"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATA_PATH = \"/content/data.json\""
      ],
      "metadata": {
        "id": "mmdTW7Es7gCz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(data_path):\n",
        "  ''' loads training dataset from json file'''\n",
        "\n",
        "  with open(data_path, \"r\") as fp:\n",
        "    data = json.load(fp)\n",
        "\n",
        "  x = np.array(data[\"mfcc\"])\n",
        "  y = np.array(data[\"labels\"])\n",
        "\n",
        "  return x,y\n"
      ],
      "metadata": {
        "id": "EeoH78a67iGJ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## dataset prepration\n",
        "\n",
        "def prepare_datasets(test_size, validation_size):\n",
        "\n",
        "  ## load data\n",
        "  x, y = load_data(DATA_PATH)\n",
        "\n",
        "  ## create train/test split\n",
        "  X_train, X_test, y_train, y_test = train_test_split(x,y, test_size=test_size)\n",
        "\n",
        "  ## trian/validation split\n",
        "  X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=validation_size)\n",
        "\n",
        "  ## the tensorflow expect 3-D array for CNN nets\n",
        "  ## 3d array --> (130, 13, 1) our expectation , where 130 is time-bins where we are taking 13 MFCCs, channel: 1\n",
        "  ## bt we have only 2D values so we have to add one extra dimension\n",
        "  X_train = X_train[..., np.newaxis]  ## give me everything we have and add new axis [...,np.newaxis]\n",
        "  X_val = X_val[...,np.newaxis]\n",
        "  X_test = X_test[...,np.newaxis]\n",
        "  ## it will 4D array --> (num_samples, 130,13,1)\n",
        "\n",
        "  return X_train, X_val, X_test, y_train, y_val, y_test\n"
      ],
      "metadata": {
        "id": "K4r7IVO7-PB7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### building the CNN model arch\n",
        "\n",
        "def build_model(input_shape):\n",
        "\n",
        "  ## create model\n",
        "  # model = keras.Sequential()\n",
        "  model = Sequential()\n",
        "\n",
        "\n",
        "  ## 1st conv layer\n",
        "  ## (kernel_num, kernel/grid_size, activation, input_shape)\n",
        "  model.add(keras.layers.Conv2D(32, (3,3), activation='relu',input_shape=input_shape))\n",
        "  # model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "\n",
        "  # ## (pooling_size, strides-by how much we move the kernel to get next value, padding-what to do at edges)\n",
        "  model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  #print(\"first layer done\")\n",
        "\n",
        "  # ## 2nd conv layer\n",
        "  # model.add(keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=input_shape))\n",
        "  model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((3,3), strides=(2,2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  #print(\"second layer done\")\n",
        "\n",
        "\n",
        "  # ## 3rd conv layer\n",
        "  # model.add(keras.layers.Conv2D(32, (2,2), activation='relu', input_shape=input_shape))\n",
        "  model.add(Conv2D(filters=32, kernel_size=(2, 2), activation='relu', input_shape=input_shape))\n",
        "  model.add(keras.layers.MaxPool2D((2,2), strides=(2,2), padding='same'))\n",
        "  model.add(keras.layers.BatchNormalization())\n",
        "\n",
        "  #print(\"third layer done\")\n",
        "\n",
        "\n",
        "  # ## flatten the output and feed it into dense layer\n",
        "  model.add(keras.layers.Flatten())\n",
        "  #print(\"flatten layer done\")\n",
        "\n",
        "\n",
        "  # ## (num_nuerons, activaton)\n",
        "  model.add(Dense(64,activation='relu'))\n",
        "  model.add(keras.layers.Dropout(0.3))                    ## to avoid over fitting\n",
        "  #print(\"fuully layer done\")\n",
        "\n",
        "  # ## output layer\n",
        "  # ## we have 10 genre - so 10 nuerons in output layer\n",
        "  model.add(Dense(10, activation='softmax'))\n",
        "  #print(\"outpurt layer done\")\n",
        "\n",
        "\n",
        "  return model\n"
      ],
      "metadata": {
        "id": "WTBc6iDRBe_p"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "  ### create train, validation and test sets\n",
        "  X_train, X_validation, X_test, y_train, y_validation, y_test = prepare_datasets(0.25, 0.2)\n",
        "\n",
        "  ## build the CNN net\n",
        "  input_shape =(X_train.shape[1], X_train.shape[2], X_train.shape[3])\n",
        "  model = build_model(input_shape)\n",
        "\n",
        "  ## compile the network\n",
        "  optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  model.compile(optimizer=optimizer,\n",
        "                loss='sparse_categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  model.summary()\n",
        "  # train the CNN\n",
        "  model.fit(X_train, y_train, validation_data=(X_validation,y_validation),\n",
        "            batch_size=32, epochs=30)\n",
        "\n",
        "  ## evaluate the CNN on the test set\n",
        "  test_error, test_accuracy = model.evaluate(X_test, y_test, verbose=1)\n",
        "  print(\"Accuracy on test set is :{}\".format(test_accuracy))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mEyQ96028YQK",
        "outputId": "b48d9ac1-2da6-404c-b9a0-f05f4776d14a"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 128, 11, 32)       320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 64, 6, 32)         0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " batch_normalization (Batch  (None, 64, 6, 32)         128       \n",
            " Normalization)                                                  \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 62, 4, 32)         9248      \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 31, 2, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_1 (Bat  (None, 31, 2, 32)         128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 30, 1, 32)         4128      \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPoolin  (None, 15, 1, 32)         0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " batch_normalization_2 (Bat  (None, 15, 1, 32)         128       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 480)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                30784     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                650       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 45514 (177.79 KB)\n",
            "Trainable params: 45322 (177.04 KB)\n",
            "Non-trainable params: 192 (768.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "19/19 [==============================] - 8s 38ms/step - loss: 3.0649 - accuracy: 0.0987 - val_loss: 3.5952 - val_accuracy: 0.0800\n",
            "Epoch 2/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 2.4853 - accuracy: 0.1839 - val_loss: 2.8416 - val_accuracy: 0.1000\n",
            "Epoch 3/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 2.2167 - accuracy: 0.2375 - val_loss: 2.4833 - val_accuracy: 0.1267\n",
            "Epoch 4/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.9959 - accuracy: 0.2993 - val_loss: 2.2534 - val_accuracy: 0.1600\n",
            "Epoch 5/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.8014 - accuracy: 0.3679 - val_loss: 2.0702 - val_accuracy: 0.2333\n",
            "Epoch 6/30\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.6705 - accuracy: 0.4030 - val_loss: 1.9333 - val_accuracy: 0.3067\n",
            "Epoch 7/30\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 1.5742 - accuracy: 0.4348 - val_loss: 1.8081 - val_accuracy: 0.3933\n",
            "Epoch 8/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.4451 - accuracy: 0.5000 - val_loss: 1.7077 - val_accuracy: 0.4067\n",
            "Epoch 9/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.3198 - accuracy: 0.5385 - val_loss: 1.6142 - val_accuracy: 0.4267\n",
            "Epoch 10/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.2547 - accuracy: 0.5418 - val_loss: 1.5322 - val_accuracy: 0.4733\n",
            "Epoch 11/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1702 - accuracy: 0.6070 - val_loss: 1.4613 - val_accuracy: 0.4800\n",
            "Epoch 12/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.1988 - accuracy: 0.5602 - val_loss: 1.3979 - val_accuracy: 0.4800\n",
            "Epoch 13/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0764 - accuracy: 0.6204 - val_loss: 1.3542 - val_accuracy: 0.5000\n",
            "Epoch 14/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0427 - accuracy: 0.6221 - val_loss: 1.3045 - val_accuracy: 0.5133\n",
            "Epoch 15/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9674 - accuracy: 0.6555 - val_loss: 1.2615 - val_accuracy: 0.5467\n",
            "Epoch 16/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 1.0072 - accuracy: 0.6488 - val_loss: 1.2172 - val_accuracy: 0.5467\n",
            "Epoch 17/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.9423 - accuracy: 0.6605 - val_loss: 1.1887 - val_accuracy: 0.5600\n",
            "Epoch 18/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.8912 - accuracy: 0.6957 - val_loss: 1.1513 - val_accuracy: 0.5800\n",
            "Epoch 19/30\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.8279 - accuracy: 0.7007 - val_loss: 1.1280 - val_accuracy: 0.5867\n",
            "Epoch 20/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.8441 - accuracy: 0.7241 - val_loss: 1.0975 - val_accuracy: 0.5933\n",
            "Epoch 21/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.7816 - accuracy: 0.7140 - val_loss: 1.0767 - val_accuracy: 0.6200\n",
            "Epoch 22/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.7479 - accuracy: 0.7575 - val_loss: 1.0507 - val_accuracy: 0.6133\n",
            "Epoch 23/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.7623 - accuracy: 0.7341 - val_loss: 1.0333 - val_accuracy: 0.6133\n",
            "Epoch 24/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.6746 - accuracy: 0.7776 - val_loss: 1.0072 - val_accuracy: 0.6467\n",
            "Epoch 25/30\n",
            "19/19 [==============================] - 0s 6ms/step - loss: 0.6714 - accuracy: 0.7625 - val_loss: 0.9812 - val_accuracy: 0.6733\n",
            "Epoch 26/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.6384 - accuracy: 0.7910 - val_loss: 0.9671 - val_accuracy: 0.6667\n",
            "Epoch 27/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.6733 - accuracy: 0.7860 - val_loss: 0.9606 - val_accuracy: 0.6667\n",
            "Epoch 28/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.6354 - accuracy: 0.7793 - val_loss: 0.9434 - val_accuracy: 0.6600\n",
            "Epoch 29/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.6201 - accuracy: 0.8027 - val_loss: 0.9354 - val_accuracy: 0.6733\n",
            "Epoch 30/30\n",
            "19/19 [==============================] - 0s 7ms/step - loss: 0.6180 - accuracy: 0.8010 - val_loss: 0.9335 - val_accuracy: 0.6533\n",
            "(250, 130, 13, 1)\n",
            "(250,)\n",
            "8/8 [==============================] - 0s 24ms/step - loss: 0.8686 - accuracy: 0.7000\n",
            "Accuracy on test set is :0.699999988079071\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## make prediction on sample\n",
        "def predict(model, X, y):\n",
        "  X = X[np.newaxis, ...]\n",
        "  predictions =  model.predict(X)  ## X ---> (130,13,1) 3-D arry , model.predict() -expects 4D array (num_samples=1, 130, 13, 1)\n",
        "\n",
        "  ## predictions we get is 2D values [[0.1, 0.2, 0.3,.....]] - 10 values the values repersents different scores for each Genre\n",
        "  ## we have predicted index : so we have to get the highest value among them\n",
        "  predict_index = np.argmax(predictions, axis=1)    ## get the index with highest value from index 1\n",
        "  print(\"Expected index :{} \\nPredicted index: {}\".format(y, predict_index))\n",
        "  return predict_index\n",
        "X = X_test[29]\n",
        "y = y_test[29]\n",
        "\n",
        "## we have 250 test-set\n",
        "\n",
        "per_i = predict(model, X, y)\n",
        "maps = [\n",
        "        \"blues\",\n",
        "        \"classical\",\n",
        "        \"country\",\n",
        "        \"disco\",\n",
        "        \"hiphop\",\n",
        "        \"jazz\",\n",
        "        \"metal\",\n",
        "        \"pop\",\n",
        "        \"reggae\",\n",
        "        \"rock\"\n",
        "    ]\n",
        "\n",
        "print(\"\\n\\nExpected Genre : {} \\nPredicted Genre: {}\".format(maps[y], maps[per_i[0]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i5uLT9GVQOVn",
        "outputId": "ea05fbaf-0daf-484d-ff5e-0c50ec5bec33"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 228ms/step\n",
            "Expected index :1 \n",
            "Predicted index: [1]\n",
            "\n",
            "\n",
            "Expected Genre : classical \n",
            "Predicted Genre: classical\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_test[101]\n",
        "y = y_test[101]\n",
        "\n",
        "## we have 250 test-set\n",
        "\n",
        "per_i = predict(model, X, y)\n",
        "print(\"\\n\\nExpected Genre : {} \\nPredicted Genre: {}\".format(maps[y], maps[per_i[0]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "an7HISE-USQh",
        "outputId": "f242fff8-8fed-467b-f323-399bea10de82"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n",
            "Expected index :8 \n",
            "Predicted index: [6]\n",
            "\n",
            "\n",
            "Expected Genre : reggae \n",
            "Predicted Genre: metal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = X_test[100]\n",
        "y = y_test[100]\n",
        "\n",
        "## we have 250 test-set\n",
        "\n",
        "per_i = predict(model, X, y)\n",
        "print(\"\\n\\nExpected Genre : {} \\nPredicted Genre: {}\".format(maps[y], maps[per_i[0]]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "areVi48pUW5V",
        "outputId": "b01b65c4-f43a-4f63-b824-52e3b7ebd8be"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 17ms/step\n",
            "Expected index :6 \n",
            "Predicted index: [6]\n",
            "\n",
            "\n",
            "Expected Genre : metal \n",
            "Predicted Genre: metal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h5-148ncUajT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}